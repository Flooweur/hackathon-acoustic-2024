{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.07240337e-06,  1.71257434e-05,  1.22597794e-05, ...,\n",
       "          8.86178623e-06,  1.40686043e-05,  1.06616626e-05],\n",
       "        [-8.23853679e-06, -8.23373648e-06, -8.74563466e-06, ...,\n",
       "          2.87394778e-06, -8.90485808e-06, -3.52464713e-06],\n",
       "        [ 6.62780906e-07, -1.40201587e-06,  1.83228190e-06, ...,\n",
       "         -1.06218545e-06,  8.53497477e-06,  2.28155182e-06],\n",
       "        [ 8.11119719e-07, -2.47451499e-06,  5.82140547e-06, ...,\n",
       "          7.73804004e-06,  2.29692650e-05,  1.10566034e-05]],\n",
       "\n",
       "       [[ 1.64466037e-05,  1.53698293e-05,  1.91599720e-05, ...,\n",
       "          1.76548729e-05,  1.70955263e-05,  1.41169767e-05],\n",
       "        [ 1.05826217e-07,  9.13145777e-06,  1.76140716e-07, ...,\n",
       "          1.21369967e-05,  1.55958387e-05,  1.23088676e-05],\n",
       "        [ 7.06836090e-06,  3.26818258e-06,  7.15508941e-06, ...,\n",
       "          4.64980394e-06,  8.96262554e-06,  6.55734584e-06],\n",
       "        [-3.18089551e-05, -8.93297238e-06, -2.51026504e-05, ...,\n",
       "         -7.02741545e-07,  1.34268867e-05,  3.24307348e-06]],\n",
       "\n",
       "       [[ 4.71092753e-05,  4.42863784e-05,  4.81883108e-05, ...,\n",
       "          5.11715079e-05,  4.54165129e-05,  4.68136932e-05],\n",
       "        [-2.25045878e-05, -1.77392903e-05, -2.29746092e-05, ...,\n",
       "         -1.16080564e-05, -1.33936874e-05, -1.49674725e-05],\n",
       "        [-7.06177207e-06, -3.24991493e-06, -3.82365158e-07, ...,\n",
       "         -2.51065717e-06, -8.64210142e-06, -1.25162501e-06],\n",
       "        [ 7.42783523e-05, -1.04295970e-04, -1.18935568e-04, ...,\n",
       "         -3.82503276e-05, -3.20346385e-06,  2.36954511e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9.19931699e-06, -1.12566586e-05, -6.25403072e-06, ...,\n",
       "         -6.18328158e-06, -7.30752890e-06, -5.60423996e-06],\n",
       "        [-8.76535523e-06, -4.28911699e-06, -5.81078848e-06, ...,\n",
       "         -3.45416925e-06, -7.85465454e-06, -3.31127535e-06],\n",
       "        [-3.47633295e-06,  1.70215810e-06, -5.54926203e-07, ...,\n",
       "          1.27369481e-06, -1.74354363e-06,  1.86796979e-06],\n",
       "        [ 2.13691851e-06,  1.01367950e-05,  1.14066588e-05, ...,\n",
       "         -1.72935925e-05, -1.24792969e-05, -1.97716799e-05]],\n",
       "\n",
       "       [[-3.31293450e-06,  8.98478265e-06,  3.39539952e-06, ...,\n",
       "         -6.63729168e-07, -1.82116059e-06, -4.22379713e-07],\n",
       "        [ 2.46191144e-06,  2.05640094e-06,  3.69124427e-06, ...,\n",
       "          3.03227239e-06,  5.80062397e-06,  2.60468755e-06],\n",
       "        [ 2.60216098e-06,  3.06390689e-06, -6.06109268e-07, ...,\n",
       "          8.91660693e-06, -2.67310270e-06,  7.77940932e-06],\n",
       "        [-1.12466259e-05, -7.33926136e-05, -1.92652078e-05, ...,\n",
       "          1.02715085e-05, -1.49204689e-05, -1.85605404e-05]],\n",
       "\n",
       "       [[-2.18067839e-06, -8.45159684e-06,  6.28021041e-07, ...,\n",
       "         -7.29226031e-06, -9.38796529e-06, -1.06312000e-05],\n",
       "        [ 5.12593260e-06,  6.90264642e-06,  6.00208568e-06, ...,\n",
       "          2.50897233e-06, -1.04963137e-05, -1.37404015e-06],\n",
       "        [ 7.75929493e-06,  1.52079519e-06,  7.64413744e-06, ...,\n",
       "          2.97681993e-07,  9.32359671e-06, -5.47995626e-07],\n",
       "        [-4.47002685e-06,  5.33304410e-06,  8.83690245e-06, ...,\n",
       "          1.00034840e-05,  4.90839875e-06,  1.29770433e-05]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv = np.load('data/LivingRoom_preprocessed_hack/Human1/deconvoled_trim.npy')\n",
    "human1_deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 667200)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2, 667200)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv_2mic = human1_deconv[:, :2, :]\n",
    "human1_deconv_2mic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3231.3293467 , -1127.87771457],\n",
       "       [-3198.54107875,  -744.5100656 ],\n",
       "       [-3192.9776274 ,  -248.26678827],\n",
       "       ...,\n",
       "       [-1717.89923578, -3166.59648491],\n",
       "       [-1808.60337549, -2779.13038427],\n",
       "       [   44.43741322,   106.48353609]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid = np.load('data/LivingRoom_preprocessed_hack/Human1/centroid.npy')\n",
    "human1_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(human1_deconv_2mic, human1_centroid, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.Tensor(X_train).cuda()\n",
    "X_test = torch.Tensor(X_test).cuda()\n",
    "\n",
    "y_train = torch.Tensor(y_train).cuda()\n",
    "y_test = torch.Tensor(y_test).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 2, 667200])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def audio_to_melspectrogram(audio, sample_rate=44100, n_mels=64, n_fft=2048, hop_length=512):\n",
    "    audio = torch.tensor(audio, dtype=torch.float32).cuda()\n",
    "    if audio.ndim == 1:\n",
    "        audio = audio.unsqueeze(0)\n",
    "    transformer = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels).cuda()\n",
    "    melspectrogram = transformer(audio)\n",
    "    return melspectrogram.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data, labels, transform=None):\n",
    "        self.audio_data = audio_data\n",
    "        _max = torch.Tensor([500, 2000]).cuda()\n",
    "        _min = torch.Tensor([-4000, -4000]).cuda()\n",
    "        self.labels = (labels - _min) / (_max - _min)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_signal = self.audio_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            audio_signal = self.transform(audio_signal)\n",
    "        return audio_signal, label\n",
    "\n",
    "def transform_to_melspectrogram(audio_signal):\n",
    "    # Convert the raw audio signal to a Mel spectrogram (or any other preprocessing)\n",
    "    melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32).cuda())\n",
    "    return melspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(X_train, y_train, transform=transform_to_melspectrogram)\n",
    "test_dataset = AudioDataset(X_test, y_test, transform=transform_to_melspectrogram)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGGishModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGishModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(2, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ).cuda()\n",
    "        # The output of features will depend on the input size, which should be calculated\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(165888, 4096),  # This dimension might need adjustment\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2),  # Assume 1000 classes; adjust as needed\n",
    "            nn.Sigmoid()\n",
    "        ).cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1).cuda()\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34159/1795162868.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32).cuda())\n",
      "/tmp/ipykernel_34159/3240000509.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.05572466768324375, Validation Loss: 0.056741717629707776\n",
      "Epoch 2, Training Loss: 0.05536507040262222, Validation Loss: 0.05619623454717489\n",
      "Epoch 3, Training Loss: 0.055435036793351176, Validation Loss: 0.05651582261690727\n",
      "Epoch 4, Training Loss: 0.05547938644886017, Validation Loss: 0.05638990150048183\n",
      "Epoch 5, Training Loss: 0.055176672488451005, Validation Loss: 0.05686721864801187\n",
      "Epoch 6, Training Loss: 0.055147481486201286, Validation Loss: 0.056249238264102205\n",
      "Epoch 7, Training Loss: 0.05510280638933182, Validation Loss: 0.0568136263352174\n",
      "Epoch 8, Training Loss: 0.05509075425565243, Validation Loss: 0.056613756773563534\n",
      "Epoch 9, Training Loss: 0.053678588941693306, Validation Loss: 0.04379569667463119\n",
      "Epoch 10, Training Loss: 0.041486500650644305, Validation Loss: 0.033153234622799434\n",
      "Epoch 11, Training Loss: 0.034487190283834936, Validation Loss: 0.032092575843517594\n",
      "Epoch 12, Training Loss: 0.03319721169769764, Validation Loss: 0.029935273700035535\n",
      "Epoch 13, Training Loss: 0.028979461919516326, Validation Loss: 0.02875122857781557\n",
      "Epoch 14, Training Loss: 0.022600733488798142, Validation Loss: 0.0198078305961994\n",
      "Epoch 15, Training Loss: 0.016864292351529003, Validation Loss: 0.01477072903743157\n",
      "Epoch 16, Training Loss: 0.012420267723500729, Validation Loss: 0.012163757131649898\n",
      "Epoch 17, Training Loss: 0.010458808979019523, Validation Loss: 0.011047214950219942\n",
      "Epoch 18, Training Loss: 0.008621223988011479, Validation Loss: 0.010170395486056805\n",
      "Epoch 19, Training Loss: 0.008259466420859098, Validation Loss: 0.013549887861769933\n",
      "Epoch 20, Training Loss: 0.008760525286197663, Validation Loss: 0.009420835663779424\n",
      "Epoch 21, Training Loss: 0.007767560863867402, Validation Loss: 0.009726440462355431\n",
      "Epoch 22, Training Loss: 0.0071914626192301516, Validation Loss: 0.008892838305865344\n",
      "Epoch 23, Training Loss: 0.006324181011877954, Validation Loss: 0.012604029562610846\n",
      "Epoch 24, Training Loss: 0.006271594502031803, Validation Loss: 0.008567744651092933\n",
      "Epoch 25, Training Loss: 0.00518115877872333, Validation Loss: 0.007919833887941562\n",
      "Epoch 26, Training Loss: 0.004888928360305727, Validation Loss: 0.007849659054325176\n",
      "Epoch 27, Training Loss: 0.005307567501440644, Validation Loss: 0.007854885744074216\n",
      "Epoch 28, Training Loss: 0.004869267693720758, Validation Loss: 0.006930851305906589\n",
      "Epoch 29, Training Loss: 0.003970754211768508, Validation Loss: 0.006768116584190955\n",
      "Epoch 30, Training Loss: 0.0037670384091325103, Validation Loss: 0.006201093550771475\n",
      "Epoch 31, Training Loss: 0.0035508321435190737, Validation Loss: 0.007862843107432127\n",
      "Epoch 32, Training Loss: 0.003363041882403195, Validation Loss: 0.006816228314374502\n",
      "Epoch 33, Training Loss: 0.00314694985980168, Validation Loss: 0.006594851111563353\n",
      "Epoch 34, Training Loss: 0.002833986592013389, Validation Loss: 0.00688736609971294\n",
      "Epoch 35, Training Loss: 0.002737301045563072, Validation Loss: 0.007252250296565203\n",
      "Epoch 36, Training Loss: 0.003308218498714268, Validation Loss: 0.005697232509891574\n",
      "Epoch 37, Training Loss: 0.0028239518799819054, Validation Loss: 0.00659200269728899\n",
      "Epoch 38, Training Loss: 0.0030874403612688186, Validation Loss: 0.006154578895522998\n",
      "Epoch 39, Training Loss: 0.0023919490142725406, Validation Loss: 0.005685026041017129\n",
      "Epoch 40, Training Loss: 0.0021273696911521256, Validation Loss: 0.006476297992496536\n",
      "Epoch 41, Training Loss: 0.0018884299544151871, Validation Loss: 0.0052033788166367095\n",
      "Epoch 42, Training Loss: 0.0022303578956052663, Validation Loss: 0.005577862020940161\n",
      "Epoch 43, Training Loss: 0.0030489635793492197, Validation Loss: 0.005335338019694273\n",
      "Epoch 44, Training Loss: 0.0018068001477513462, Validation Loss: 0.006230849790601776\n",
      "Epoch 45, Training Loss: 0.0018918532761745155, Validation Loss: 0.005806146485086244\n",
      "Epoch 46, Training Loss: 0.0019270202785264701, Validation Loss: 0.005265796145137686\n",
      "Epoch 47, Training Loss: 0.0017317298043053597, Validation Loss: 0.0054606587196198795\n",
      "Epoch 48, Training Loss: 0.0018120344157796352, Validation Loss: 0.006012346953726732\n",
      "Epoch 49, Training Loss: 0.0016443870472721755, Validation Loss: 0.005283877486363053\n",
      "Epoch 50, Training Loss: 0.0014377536973915995, Validation Loss: 0.005677055549592926\n",
      "Epoch 51, Training Loss: 0.0013914448133436964, Validation Loss: 0.0053319504591994565\n",
      "Epoch 52, Training Loss: 0.0013319098425563424, Validation Loss: 0.005332212805604706\n",
      "Epoch 53, Training Loss: 0.0013704584911465646, Validation Loss: 0.005468431591557769\n",
      "Epoch 54, Training Loss: 0.0014492387406062335, Validation Loss: 0.0050642694561527325\n",
      "Epoch 55, Training Loss: 0.0013184025406371802, Validation Loss: 0.005177463816765409\n",
      "Epoch 56, Training Loss: 0.0012488531385315583, Validation Loss: 0.005018738922304832\n",
      "Epoch 57, Training Loss: 0.0011935449193697422, Validation Loss: 0.00479429501753587\n",
      "Epoch 58, Training Loss: 0.0011579892784357071, Validation Loss: 0.006172019039065792\n",
      "Epoch 59, Training Loss: 0.0015120378450956195, Validation Loss: 0.004897342923168953\n",
      "Epoch 60, Training Loss: 0.0013588278216775506, Validation Loss: 0.005119779523318777\n",
      "Epoch 61, Training Loss: 0.0011113195074722172, Validation Loss: 0.0047355647771977465\n",
      "Epoch 62, Training Loss: 0.0010911139252129942, Validation Loss: 0.004838666609989909\n",
      "Epoch 63, Training Loss: 0.0009874068829230963, Validation Loss: 0.004595796040330942\n",
      "Epoch 64, Training Loss: 0.0011774772091303022, Validation Loss: 0.005726042651356413\n",
      "Epoch 65, Training Loss: 0.0012744549545459448, Validation Loss: 0.004883687441738753\n",
      "Epoch 66, Training Loss: 0.001222981532337144, Validation Loss: 0.004592676360446673\n",
      "Epoch 67, Training Loss: 0.0009457078279228881, Validation Loss: 0.004586696194914671\n",
      "Epoch 68, Training Loss: 0.0007928869331954047, Validation Loss: 0.004452244450266545\n",
      "Epoch 69, Training Loss: 0.0009862368000904099, Validation Loss: 0.004717639987715161\n",
      "Epoch 70, Training Loss: 0.0012308727472554893, Validation Loss: 0.0048303492725468595\n",
      "Epoch 71, Training Loss: 0.0011425419501028954, Validation Loss: 0.0047331046527968\n",
      "Epoch 72, Training Loss: 0.0010222672671079637, Validation Loss: 0.004494226089893625\n",
      "Epoch 73, Training Loss: 0.0008846535533666611, Validation Loss: 0.0047605820179272154\n",
      "Epoch 74, Training Loss: 0.0009602346713654696, Validation Loss: 0.004827140561806468\n",
      "Epoch 75, Training Loss: 0.0009090325731085613, Validation Loss: 0.00443432189954015\n",
      "Epoch 76, Training Loss: 0.000801174640073441, Validation Loss: 0.004450693332518523\n",
      "Epoch 77, Training Loss: 0.0008044548332691193, Validation Loss: 0.004528035373928456\n",
      "Epoch 78, Training Loss: 0.0009798023256007583, Validation Loss: 0.004443234692399318\n",
      "Epoch 79, Training Loss: 0.001146388613851741, Validation Loss: 0.004843782973833955\n",
      "Epoch 80, Training Loss: 0.0008394343353575096, Validation Loss: 0.004294211958320095\n",
      "Epoch 81, Training Loss: 0.0007893728377530351, Validation Loss: 0.004725684501373997\n",
      "Epoch 82, Training Loss: 0.0006567950913449749, Validation Loss: 0.004201871289226871\n",
      "Epoch 83, Training Loss: 0.0007040698191849515, Validation Loss: 0.00413342325303417\n",
      "Epoch 84, Training Loss: 0.0006907548586605116, Validation Loss: 0.00460089288222102\n",
      "Epoch 85, Training Loss: 0.0007471914880443364, Validation Loss: 0.004792634541025529\n",
      "Epoch 86, Training Loss: 0.0007472546491771936, Validation Loss: 0.004580357774662284\n",
      "Epoch 87, Training Loss: 0.0006821527431020514, Validation Loss: 0.004152077393463025\n",
      "Epoch 88, Training Loss: 0.0005899255158146843, Validation Loss: 0.0041239781555934595\n",
      "Epoch 89, Training Loss: 0.0007869212795048952, Validation Loss: 0.005094361222850589\n",
      "Epoch 90, Training Loss: 0.0008659181313123554, Validation Loss: 0.00408928209127715\n",
      "Epoch 91, Training Loss: 0.0007364498008973897, Validation Loss: 0.004341823389180577\n",
      "Epoch 92, Training Loss: 0.0006852083856938407, Validation Loss: 0.004316791390570311\n",
      "Epoch 93, Training Loss: 0.0009273365151602775, Validation Loss: 0.004868257557973266\n",
      "Epoch 94, Training Loss: 0.0010245313285849988, Validation Loss: 0.004274272861389013\n",
      "Epoch 95, Training Loss: 0.0008030902489554137, Validation Loss: 0.004244379967880936\n",
      "Epoch 96, Training Loss: 0.0007413635490229353, Validation Loss: 0.004208336315619258\n",
      "Epoch 97, Training Loss: 0.0006503194983815774, Validation Loss: 0.003927192328354487\n",
      "Epoch 98, Training Loss: 0.0007097104127751664, Validation Loss: 0.004485204690494216\n",
      "Epoch 99, Training Loss: 0.0007978777668904513, Validation Loss: 0.004487278172746301\n",
      "Epoch 100, Training Loss: 0.0008261442166985944, Validation Loss: 0.004608561804231543\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGGishModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for spectrograms, coordinates in train_loader:\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        coordinates = coordinates.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, coordinates)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, coordinates in test_loader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            coordinates = coordinates.to(device)\n",
    "\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, coordinates)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}, Validation Loss: {validation_loss / len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_34159/1795162868.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32).cuda())\n",
      "/tmp/ipykernel_34159/3240000509.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distance Error: 202.3763176243583\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "_max = np.array([500, 2000])\n",
    "_min = np.array([-4000, -4000])\n",
    "\n",
    "def unnormalize(x):\n",
    "    return (x + 1) / 2 * (_max - _min) + _min\n",
    "\n",
    "distance_errors = []\n",
    "\n",
    "for spectrograms, actual_coordinates in test_loader:\n",
    "    spectrograms = spectrograms.to(device)\n",
    "    actual_coordinates = actual_coordinates.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_coordinates = model(spectrograms)\n",
    "        predicted_coordinates = predicted_coordinates.cpu().numpy()\n",
    "        actual_coordinates = actual_coordinates.cpu().numpy()\n",
    "    \n",
    "    for pred, actual in zip(predicted_coordinates, actual_coordinates):\n",
    "        pred_unnorm = unnormalize(pred)\n",
    "        actual_unnorm = unnormalize(actual)\n",
    "        \n",
    "        distance = np.linalg.norm(pred_unnorm - actual_unnorm)\n",
    "        distance_errors.append(distance)\n",
    "\n",
    "average_distance_error = np.mean(distance_errors)\n",
    "\n",
    "print(\"Average Distance Error:\", average_distance_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
