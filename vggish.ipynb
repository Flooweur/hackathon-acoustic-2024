{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.07240337e-06,  1.71257434e-05,  1.22597794e-05, ...,\n",
       "          8.86178623e-06,  1.40686043e-05,  1.06616626e-05],\n",
       "        [-8.23853679e-06, -8.23373648e-06, -8.74563466e-06, ...,\n",
       "          2.87394778e-06, -8.90485808e-06, -3.52464713e-06],\n",
       "        [ 6.62780906e-07, -1.40201587e-06,  1.83228190e-06, ...,\n",
       "         -1.06218545e-06,  8.53497477e-06,  2.28155182e-06],\n",
       "        [ 8.11119719e-07, -2.47451499e-06,  5.82140547e-06, ...,\n",
       "          7.73804004e-06,  2.29692650e-05,  1.10566034e-05]],\n",
       "\n",
       "       [[ 1.64466037e-05,  1.53698293e-05,  1.91599720e-05, ...,\n",
       "          1.76548729e-05,  1.70955263e-05,  1.41169767e-05],\n",
       "        [ 1.05826217e-07,  9.13145777e-06,  1.76140716e-07, ...,\n",
       "          1.21369967e-05,  1.55958387e-05,  1.23088676e-05],\n",
       "        [ 7.06836090e-06,  3.26818258e-06,  7.15508941e-06, ...,\n",
       "          4.64980394e-06,  8.96262554e-06,  6.55734584e-06],\n",
       "        [-3.18089551e-05, -8.93297238e-06, -2.51026504e-05, ...,\n",
       "         -7.02741545e-07,  1.34268867e-05,  3.24307348e-06]],\n",
       "\n",
       "       [[ 4.71092753e-05,  4.42863784e-05,  4.81883108e-05, ...,\n",
       "          5.11715079e-05,  4.54165129e-05,  4.68136932e-05],\n",
       "        [-2.25045878e-05, -1.77392903e-05, -2.29746092e-05, ...,\n",
       "         -1.16080564e-05, -1.33936874e-05, -1.49674725e-05],\n",
       "        [-7.06177207e-06, -3.24991493e-06, -3.82365158e-07, ...,\n",
       "         -2.51065717e-06, -8.64210142e-06, -1.25162501e-06],\n",
       "        [ 7.42783523e-05, -1.04295970e-04, -1.18935568e-04, ...,\n",
       "         -3.82503276e-05, -3.20346385e-06,  2.36954511e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9.19931699e-06, -1.12566586e-05, -6.25403072e-06, ...,\n",
       "         -6.18328158e-06, -7.30752890e-06, -5.60423996e-06],\n",
       "        [-8.76535523e-06, -4.28911699e-06, -5.81078848e-06, ...,\n",
       "         -3.45416925e-06, -7.85465454e-06, -3.31127535e-06],\n",
       "        [-3.47633295e-06,  1.70215810e-06, -5.54926203e-07, ...,\n",
       "          1.27369481e-06, -1.74354363e-06,  1.86796979e-06],\n",
       "        [ 2.13691851e-06,  1.01367950e-05,  1.14066588e-05, ...,\n",
       "         -1.72935925e-05, -1.24792969e-05, -1.97716799e-05]],\n",
       "\n",
       "       [[-3.31293450e-06,  8.98478265e-06,  3.39539952e-06, ...,\n",
       "         -6.63729168e-07, -1.82116059e-06, -4.22379713e-07],\n",
       "        [ 2.46191144e-06,  2.05640094e-06,  3.69124427e-06, ...,\n",
       "          3.03227239e-06,  5.80062397e-06,  2.60468755e-06],\n",
       "        [ 2.60216098e-06,  3.06390689e-06, -6.06109268e-07, ...,\n",
       "          8.91660693e-06, -2.67310270e-06,  7.77940932e-06],\n",
       "        [-1.12466259e-05, -7.33926136e-05, -1.92652078e-05, ...,\n",
       "          1.02715085e-05, -1.49204689e-05, -1.85605404e-05]],\n",
       "\n",
       "       [[-2.18067839e-06, -8.45159684e-06,  6.28021041e-07, ...,\n",
       "         -7.29226031e-06, -9.38796529e-06, -1.06312000e-05],\n",
       "        [ 5.12593260e-06,  6.90264642e-06,  6.00208568e-06, ...,\n",
       "          2.50897233e-06, -1.04963137e-05, -1.37404015e-06],\n",
       "        [ 7.75929493e-06,  1.52079519e-06,  7.64413744e-06, ...,\n",
       "          2.97681993e-07,  9.32359671e-06, -5.47995626e-07],\n",
       "        [-4.47002685e-06,  5.33304410e-06,  8.83690245e-06, ...,\n",
       "          1.00034840e-05,  4.90839875e-06,  1.29770433e-05]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv = np.load('data/LivingRoom_preprocessed_hack/Human1/deconvoled_trim.npy')\n",
    "human1_deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 667200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux-6.1.0-16-amd64-x86_64-with-glibc2.35'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3231.3293467 , -1127.87771457],\n",
       "       [-3198.54107875,  -744.5100656 ],\n",
       "       [-3192.9776274 ,  -248.26678827],\n",
       "       ...,\n",
       "       [-1717.89923578, -3166.59648491],\n",
       "       [-1808.60337549, -2779.13038427],\n",
       "       [   44.43741322,   106.48353609]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid = np.load('data/LivingRoom_preprocessed_hack/Human1/centroid.npy')\n",
    "human1_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(human1_deconv, human1_centroid, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def audio_to_melspectrogram(audio, sample_rate=44100, n_mels=64, n_fft=2048, hop_length=512):\n",
    "    audio = torch.tensor(audio, dtype=torch.float32)\n",
    "    if audio.ndim == 1:\n",
    "        audio = audio.unsqueeze(0)\n",
    "    transformer = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    melspectrogram = transformer(audio)\n",
    "    return melspectrogram.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data, labels, transform=None):\n",
    "        self.audio_data = audio_data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_signal = self.audio_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            audio_signal = self.transform(audio_signal)\n",
    "        return audio_signal, label\n",
    "\n",
    "def transform_to_melspectrogram(audio_signal):\n",
    "    # Convert the raw audio signal to a Mel spectrogram (or any other preprocessing)\n",
    "    melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32))\n",
    "    return melspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(X_train, y_train, transform=transform_to_melspectrogram)\n",
    "test_dataset = AudioDataset(X_test, y_test, transform=transform_to_melspectrogram)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGGishModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGishModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(165888, 4096)\n",
    "        self.fc2 = nn.Linear(4096, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 2)  # Outputs the coordinates\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv4(x)), 2)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGGishModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGishModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(4, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # The output of features will depend on the input size, which should be calculated\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(165888, 4096),  # This dimension might need adjustment\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2),  # Assume 1000 classes; adjust as needed\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43191/4051609367.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32))\n",
      "/tmp/ipykernel_43191/2110669507.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 3863751.9625, Validation Loss: 4224984.9625\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGGishModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for spectrograms, coordinates in train_loader:\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        coordinates = coordinates.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, coordinates)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, coordinates in test_loader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            coordinates = coordinates.to(device)\n",
    "\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, coordinates)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}, Validation Loss: {validation_loss / len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Coordinate: [[-2153.3108 -1000.0165]]\n",
      "Actual Coordinate: [-2082.7336   -561.10864]\n",
      "Distance 984158.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9158/4051609367.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32))\n",
      "/tmp/ipykernel_9158/2110669507.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Assuming your train_loader is already created and ready to use\n",
    "# Retrieve the first batch from the DataLoader\n",
    "first_batch = next(iter(train_loader))\n",
    "spectrograms, actual_coordinates = first_batch\n",
    "\n",
    "# Extract only the first element from the batch\n",
    "first_spectrogram = spectrograms[0].unsqueeze(0).to(device)  # Add a batch dimension\n",
    "first_actual_coordinate = actual_coordinates[0].to(device)\n",
    "with torch.no_grad():  # We do not need to track gradients here\n",
    "    predicted_coordinate = model(first_spectrogram)\n",
    "    predicted_coordinate = predicted_coordinate.cpu().numpy()  # Move data to cpu and convert to numpy array for easy handling\n",
    "\n",
    "print(\"Predicted Coordinate:\", predicted_coordinate)\n",
    "print(\"Actual Coordinate:\", first_actual_coordinate.cpu().numpy())  # Also convert actual coordinates to numpy array for consistency\n",
    "print(\"Distance\", (predicted_coordinate ** 2).sum() - (first_actual_coordinate.cpu().numpy() ** 2).sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
