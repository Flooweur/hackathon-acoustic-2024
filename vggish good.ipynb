{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.07240337e-06,  1.71257434e-05,  1.22597794e-05, ...,\n",
       "          8.86178623e-06,  1.40686043e-05,  1.06616626e-05],\n",
       "        [-8.23853679e-06, -8.23373648e-06, -8.74563466e-06, ...,\n",
       "          2.87394778e-06, -8.90485808e-06, -3.52464713e-06],\n",
       "        [ 6.62780906e-07, -1.40201587e-06,  1.83228190e-06, ...,\n",
       "         -1.06218545e-06,  8.53497477e-06,  2.28155182e-06],\n",
       "        [ 8.11119719e-07, -2.47451499e-06,  5.82140547e-06, ...,\n",
       "          7.73804004e-06,  2.29692650e-05,  1.10566034e-05]],\n",
       "\n",
       "       [[ 1.64466037e-05,  1.53698293e-05,  1.91599720e-05, ...,\n",
       "          1.76548729e-05,  1.70955263e-05,  1.41169767e-05],\n",
       "        [ 1.05826217e-07,  9.13145777e-06,  1.76140716e-07, ...,\n",
       "          1.21369967e-05,  1.55958387e-05,  1.23088676e-05],\n",
       "        [ 7.06836090e-06,  3.26818258e-06,  7.15508941e-06, ...,\n",
       "          4.64980394e-06,  8.96262554e-06,  6.55734584e-06],\n",
       "        [-3.18089551e-05, -8.93297238e-06, -2.51026504e-05, ...,\n",
       "         -7.02741545e-07,  1.34268867e-05,  3.24307348e-06]],\n",
       "\n",
       "       [[ 4.71092753e-05,  4.42863784e-05,  4.81883108e-05, ...,\n",
       "          5.11715079e-05,  4.54165129e-05,  4.68136932e-05],\n",
       "        [-2.25045878e-05, -1.77392903e-05, -2.29746092e-05, ...,\n",
       "         -1.16080564e-05, -1.33936874e-05, -1.49674725e-05],\n",
       "        [-7.06177207e-06, -3.24991493e-06, -3.82365158e-07, ...,\n",
       "         -2.51065717e-06, -8.64210142e-06, -1.25162501e-06],\n",
       "        [ 7.42783523e-05, -1.04295970e-04, -1.18935568e-04, ...,\n",
       "         -3.82503276e-05, -3.20346385e-06,  2.36954511e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9.19931699e-06, -1.12566586e-05, -6.25403072e-06, ...,\n",
       "         -6.18328158e-06, -7.30752890e-06, -5.60423996e-06],\n",
       "        [-8.76535523e-06, -4.28911699e-06, -5.81078848e-06, ...,\n",
       "         -3.45416925e-06, -7.85465454e-06, -3.31127535e-06],\n",
       "        [-3.47633295e-06,  1.70215810e-06, -5.54926203e-07, ...,\n",
       "          1.27369481e-06, -1.74354363e-06,  1.86796979e-06],\n",
       "        [ 2.13691851e-06,  1.01367950e-05,  1.14066588e-05, ...,\n",
       "         -1.72935925e-05, -1.24792969e-05, -1.97716799e-05]],\n",
       "\n",
       "       [[-3.31293450e-06,  8.98478265e-06,  3.39539952e-06, ...,\n",
       "         -6.63729168e-07, -1.82116059e-06, -4.22379713e-07],\n",
       "        [ 2.46191144e-06,  2.05640094e-06,  3.69124427e-06, ...,\n",
       "          3.03227239e-06,  5.80062397e-06,  2.60468755e-06],\n",
       "        [ 2.60216098e-06,  3.06390689e-06, -6.06109268e-07, ...,\n",
       "          8.91660693e-06, -2.67310270e-06,  7.77940932e-06],\n",
       "        [-1.12466259e-05, -7.33926136e-05, -1.92652078e-05, ...,\n",
       "          1.02715085e-05, -1.49204689e-05, -1.85605404e-05]],\n",
       "\n",
       "       [[-2.18067839e-06, -8.45159684e-06,  6.28021041e-07, ...,\n",
       "         -7.29226031e-06, -9.38796529e-06, -1.06312000e-05],\n",
       "        [ 5.12593260e-06,  6.90264642e-06,  6.00208568e-06, ...,\n",
       "          2.50897233e-06, -1.04963137e-05, -1.37404015e-06],\n",
       "        [ 7.75929493e-06,  1.52079519e-06,  7.64413744e-06, ...,\n",
       "          2.97681993e-07,  9.32359671e-06, -5.47995626e-07],\n",
       "        [-4.47002685e-06,  5.33304410e-06,  8.83690245e-06, ...,\n",
       "          1.00034840e-05,  4.90839875e-06,  1.29770433e-05]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv = np.load('data/LivingRoom_preprocessed_hack/Human1/deconvoled_trim.npy')\n",
    "human1_deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 667200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux-6.1.0-17-amd64-x86_64-with-glibc2.35'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3231.3293467 , -1127.87771457],\n",
       "       [-3198.54107875,  -744.5100656 ],\n",
       "       [-3192.9776274 ,  -248.26678827],\n",
       "       ...,\n",
       "       [-1717.89923578, -3166.59648491],\n",
       "       [-1808.60337549, -2779.13038427],\n",
       "       [   44.43741322,   106.48353609]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid = np.load('data/LivingRoom_preprocessed_hack/Human1/centroid.npy')\n",
    "human1_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(human1_deconv, human1_centroid, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "\n",
    "y_train = torch.Tensor(y_train)\n",
    "y_test = torch.Tensor(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def audio_to_melspectrogram(audio, sample_rate=44100, n_mels=64, n_fft=2048, hop_length=512):\n",
    "    audio = torch.tensor(audio, dtype=torch.float32)\n",
    "    if audio.ndim == 1:\n",
    "        audio = audio.unsqueeze(0)\n",
    "    transformer = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels)\n",
    "    melspectrogram = transformer(audio)\n",
    "    return melspectrogram.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data, labels, transform=None):\n",
    "        self.audio_data = audio_data\n",
    "        _max = torch.Tensor([500, 2000])\n",
    "        _min = torch.Tensor([-4000, -4000])\n",
    "        self.labels = (labels - _min) / (_max - _min)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_signal = self.audio_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            audio_signal = self.transform(audio_signal)\n",
    "        return audio_signal, label\n",
    "\n",
    "def transform_to_melspectrogram(audio_signal):\n",
    "    # Convert the raw audio signal to a Mel spectrogram (or any other preprocessing)\n",
    "    melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32))\n",
    "    return melspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(X_train, y_train, transform=transform_to_melspectrogram)\n",
    "test_dataset = AudioDataset(X_test, y_test, transform=transform_to_melspectrogram)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGGishModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGishModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(4, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        # The output of features will depend on the input size, which should be calculated\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(165888, 4096),  # This dimension might need adjustment\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2),  # Assume 1000 classes; adjust as needed\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3719/1710517960.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32))\n",
      "/tmp/ipykernel_3719/371298476.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.055613991320133206, Validation Loss: 0.057278584115780316\n",
      "Epoch 2, Training Loss: 0.05563448302447796, Validation Loss: 0.05645810497494844\n",
      "Epoch 3, Training Loss: 0.05518368162214756, Validation Loss: 0.05702823916306862\n",
      "Epoch 4, Training Loss: 0.055159759148955344, Validation Loss: 0.056555587225235425\n",
      "Epoch 5, Training Loss: 0.05522705659270286, Validation Loss: 0.05637057956594687\n",
      "Epoch 6, Training Loss: 0.05512250527739525, Validation Loss: 0.056269173725293234\n",
      "Epoch 7, Training Loss: 0.055156776830554005, Validation Loss: 0.056676825938316494\n",
      "Epoch 8, Training Loss: 0.0553509933501482, Validation Loss: 0.0567192779137538\n",
      "Epoch 9, Training Loss: 0.054627205356955526, Validation Loss: 0.05411213579086157\n",
      "Epoch 10, Training Loss: 0.04982207238674164, Validation Loss: 0.05180693675692265\n",
      "Epoch 11, Training Loss: 0.044502016454935074, Validation Loss: 0.04217800755913441\n",
      "Epoch 12, Training Loss: 0.03877424459904432, Validation Loss: 0.03467505589987223\n",
      "Epoch 13, Training Loss: 0.034352073594927784, Validation Loss: 0.03273542583561861\n",
      "Epoch 14, Training Loss: 0.027773929238319398, Validation Loss: 0.033709355271779574\n",
      "Epoch 15, Training Loss: 0.021924764644354583, Validation Loss: 0.020803790611143295\n",
      "Epoch 16, Training Loss: 0.018473297506570816, Validation Loss: 0.01937529516334717\n",
      "Epoch 17, Training Loss: 0.017196012269705532, Validation Loss: 0.015530414807681855\n",
      "Epoch 18, Training Loss: 0.011857634009793401, Validation Loss: 0.012538051089415183\n",
      "Epoch 19, Training Loss: 0.012651692377403378, Validation Loss: 0.011936830679098\n",
      "Epoch 20, Training Loss: 0.010061728619039059, Validation Loss: 0.011228882027073549\n",
      "Epoch 21, Training Loss: 0.00941260512918234, Validation Loss: 0.009747363519496642\n",
      "Epoch 22, Training Loss: 0.008771788752637804, Validation Loss: 0.008955254076192012\n",
      "Epoch 23, Training Loss: 0.007601698115468025, Validation Loss: 0.009604285972622724\n",
      "Epoch 24, Training Loss: 0.007240244154818356, Validation Loss: 0.008928842018716611\n",
      "Epoch 25, Training Loss: 0.0067881467984989285, Validation Loss: 0.008080740566723622\n",
      "Epoch 26, Training Loss: 0.005809609699063003, Validation Loss: 0.008295574655326514\n",
      "Epoch 27, Training Loss: 0.006313804336823523, Validation Loss: 0.007618761764696011\n",
      "Epoch 28, Training Loss: 0.005216456572525203, Validation Loss: 0.007204664047234333\n",
      "Epoch 29, Training Loss: 0.005207323832437396, Validation Loss: 0.007137557491660118\n",
      "Epoch 30, Training Loss: 0.004306577064562589, Validation Loss: 0.00692917238204525\n",
      "Epoch 31, Training Loss: 0.004107408137060702, Validation Loss: 0.006405352274529063\n",
      "Epoch 32, Training Loss: 0.00433415746781975, Validation Loss: 0.0070060823972408585\n",
      "Epoch 33, Training Loss: 0.004159982681740075, Validation Loss: 0.006180524951420152\n",
      "Epoch 34, Training Loss: 0.003367818791884929, Validation Loss: 0.0058686579398524305\n",
      "Epoch 35, Training Loss: 0.00362484201323241, Validation Loss: 0.0053928249085751865\n",
      "Epoch 36, Training Loss: 0.0033271684078499675, Validation Loss: 0.006135335854756145\n",
      "Epoch 37, Training Loss: 0.00302039529196918, Validation Loss: 0.00532464265751724\n",
      "Epoch 38, Training Loss: 0.0028335543046705425, Validation Loss: 0.004948731010349898\n",
      "Epoch 39, Training Loss: 0.002454124251380563, Validation Loss: 0.005326938600494311\n",
      "Epoch 40, Training Loss: 0.002822116108145565, Validation Loss: 0.006069815466896846\n",
      "Epoch 41, Training Loss: 0.0027325540455058217, Validation Loss: 0.0053562057885126425\n",
      "Epoch 42, Training Loss: 0.0022110319300554693, Validation Loss: 0.004853068904664654\n",
      "Epoch 43, Training Loss: 0.002509745811112225, Validation Loss: 0.005052817663034568\n",
      "Epoch 44, Training Loss: 0.002594909055624157, Validation Loss: 0.004893939368999922\n",
      "Epoch 45, Training Loss: 0.002333052430767566, Validation Loss: 0.004960877068627339\n",
      "Epoch 46, Training Loss: 0.0021306327264755966, Validation Loss: 0.004414996839701557\n",
      "Epoch 47, Training Loss: 0.002126744717825204, Validation Loss: 0.004682180829919302\n",
      "Epoch 48, Training Loss: 0.0019945883087348192, Validation Loss: 0.0044239857950462745\n",
      "Epoch 49, Training Loss: 0.0018929225218016655, Validation Loss: 0.004612973927018734\n",
      "Epoch 50, Training Loss: 0.0018949051585514097, Validation Loss: 0.005533688567363872\n",
      "Epoch 51, Training Loss: 0.0017936581454705447, Validation Loss: 0.003813857982794826\n",
      "Epoch 52, Training Loss: 0.0016632336715701967, Validation Loss: 0.00460382833933601\n",
      "Epoch 53, Training Loss: 0.001721762386150658, Validation Loss: 0.00399574929346832\n",
      "Epoch 54, Training Loss: 0.001544172781286761, Validation Loss: 0.0036782273897328055\n",
      "Epoch 55, Training Loss: 0.0016720547049771995, Validation Loss: 0.0037891646094906787\n",
      "Epoch 56, Training Loss: 0.0012951297138351946, Validation Loss: 0.003679296548048464\n",
      "Epoch 57, Training Loss: 0.0012913590541575105, Validation Loss: 0.004035601231197898\n",
      "Epoch 58, Training Loss: 0.0012752504588570445, Validation Loss: 0.003780887695029378\n",
      "Epoch 59, Training Loss: 0.0013451766269281507, Validation Loss: 0.003748993702734319\n",
      "Epoch 60, Training Loss: 0.0012689484818838538, Validation Loss: 0.003727246254969102\n",
      "Epoch 61, Training Loss: 0.0012009685195516794, Validation Loss: 0.0035310283554001497\n",
      "Epoch 62, Training Loss: 0.0010800007835496217, Validation Loss: 0.003640103583725599\n",
      "Epoch 63, Training Loss: 0.001146777035901323, Validation Loss: 0.004011277682506121\n",
      "Epoch 64, Training Loss: 0.0011655414558481424, Validation Loss: 0.004303025905616009\n",
      "Epoch 65, Training Loss: 0.001207029044162482, Validation Loss: 0.003305571055254684\n",
      "Epoch 66, Training Loss: 0.0011847946525085718, Validation Loss: 0.003338870099888971\n",
      "Epoch 67, Training Loss: 0.0010369147174060344, Validation Loss: 0.0031833429283534107\n",
      "Epoch 68, Training Loss: 0.0010212629090528935, Validation Loss: 0.0037870384000528315\n",
      "Epoch 69, Training Loss: 0.0010619455517735332, Validation Loss: 0.004054490029095457\n",
      "Epoch 70, Training Loss: 0.001263482834910974, Validation Loss: 0.0041843523235561755\n",
      "Epoch 71, Training Loss: 0.0010862877999898047, Validation Loss: 0.0030711097821879843\n",
      "Epoch 72, Training Loss: 0.0012003388692392036, Validation Loss: 0.00386160547630145\n",
      "Epoch 73, Training Loss: 0.0013136909785680473, Validation Loss: 0.003222360141360416\n",
      "Epoch 74, Training Loss: 0.000955393192707561, Validation Loss: 0.0034648701548576355\n",
      "Epoch 75, Training Loss: 0.0009746343968436122, Validation Loss: 0.0032321771678443137\n",
      "Epoch 76, Training Loss: 0.0008883791824337095, Validation Loss: 0.0031004361629199525\n",
      "Epoch 77, Training Loss: 0.0008886725234333425, Validation Loss: 0.0032251814577298667\n",
      "Epoch 78, Training Loss: 0.001017379699042067, Validation Loss: 0.0033728789197854125\n",
      "Epoch 79, Training Loss: 0.0009678829711629078, Validation Loss: 0.0034094430291308807\n",
      "Epoch 80, Training Loss: 0.0009543440985726192, Validation Loss: 0.0029974812641739845\n",
      "Epoch 81, Training Loss: 0.0007911659503588453, Validation Loss: 0.003155994820050322\n",
      "Epoch 82, Training Loss: 0.0009210090490523726, Validation Loss: 0.003195391564916533\n",
      "Epoch 83, Training Loss: 0.0007338988641276956, Validation Loss: 0.0030305328725192407\n",
      "Epoch 84, Training Loss: 0.001090753326425329, Validation Loss: 0.004098417497096727\n",
      "Epoch 85, Training Loss: 0.001406579181784764, Validation Loss: 0.0031529290225500097\n",
      "Epoch 86, Training Loss: 0.0007999236509203911, Validation Loss: 0.0028611944379428257\n",
      "Epoch 87, Training Loss: 0.00074869881675113, Validation Loss: 0.0030553315210944186\n",
      "Epoch 88, Training Loss: 0.0007065270049497485, Validation Loss: 0.0027743799325365285\n",
      "Epoch 89, Training Loss: 0.0008191831392468885, Validation Loss: 0.00303305487614125\n",
      "Epoch 90, Training Loss: 0.000784361672704108, Validation Loss: 0.0029902509413659573\n",
      "Epoch 91, Training Loss: 0.0006826854421524331, Validation Loss: 0.0037513997477407637\n",
      "Epoch 92, Training Loss: 0.0007870469277258962, Validation Loss: 0.0030156264189057625\n",
      "Epoch 93, Training Loss: 0.0007026534987380728, Validation Loss: 0.0028845373218735824\n",
      "Epoch 94, Training Loss: 0.0008294956741156056, Validation Loss: 0.0026318464929667804\n",
      "Epoch 95, Training Loss: 0.0008912269154097885, Validation Loss: 0.0030607600123263323\n",
      "Epoch 96, Training Loss: 0.0006567889131838455, Validation Loss: 0.003048634004349319\n",
      "Epoch 97, Training Loss: 0.0007369993097381667, Validation Loss: 0.0025370048153070877\n",
      "Epoch 98, Training Loss: 0.0006028188252821565, Validation Loss: 0.002520637264331946\n",
      "Epoch 99, Training Loss: 0.000609400964458473, Validation Loss: 0.002789794637535054\n",
      "Epoch 100, Training Loss: 0.0007641201186925173, Validation Loss: 0.0028627527286656774\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGGishModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for spectrograms, coordinates in train_loader:\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        coordinates = coordinates.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, coordinates)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, coordinates in test_loader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            coordinates = coordinates.to(device)\n",
    "\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, coordinates)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}, Validation Loss: {validation_loss / len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 2\n",
    "first_spectogram, first_actual_coordinate = (X_test[i], y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3719/1710517960.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32))\n",
      "/tmp/ipykernel_3719/371298476.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distance Error: 168.32660961884562\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "_max = np.array([500, 2000])\n",
    "_min = np.array([-4000, -4000])\n",
    "\n",
    "def unnormalize(x):\n",
    "    return (x + 1) / 2 * (_max - _min) + _min\n",
    "\n",
    "distance_errors = []\n",
    "\n",
    "for spectrograms, actual_coordinates in test_loader:\n",
    "    spectrograms = spectrograms.to(device)\n",
    "    actual_coordinates = actual_coordinates.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_coordinates = model(spectrograms)\n",
    "        predicted_coordinates = predicted_coordinates.cpu().numpy()\n",
    "        actual_coordinates = actual_coordinates.cpu().numpy()\n",
    "    \n",
    "    for pred, actual in zip(predicted_coordinates, actual_coordinates):\n",
    "        pred_unnorm = unnormalize(pred)\n",
    "        actual_unnorm = unnormalize(actual)\n",
    "        \n",
    "        distance = np.linalg.norm(pred_unnorm - actual_unnorm)\n",
    "        distance_errors.append(distance)\n",
    "\n",
    "average_distance_error = np.mean(distance_errors)\n",
    "\n",
    "print(\"Average Distance Error:\", average_distance_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

