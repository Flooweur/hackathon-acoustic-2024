{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 9.07240337e-06,  1.71257434e-05,  1.22597794e-05, ...,\n",
       "          8.86178623e-06,  1.40686043e-05,  1.06616626e-05],\n",
       "        [-8.23853679e-06, -8.23373648e-06, -8.74563466e-06, ...,\n",
       "          2.87394778e-06, -8.90485808e-06, -3.52464713e-06],\n",
       "        [ 6.62780906e-07, -1.40201587e-06,  1.83228190e-06, ...,\n",
       "         -1.06218545e-06,  8.53497477e-06,  2.28155182e-06],\n",
       "        [ 8.11119719e-07, -2.47451499e-06,  5.82140547e-06, ...,\n",
       "          7.73804004e-06,  2.29692650e-05,  1.10566034e-05]],\n",
       "\n",
       "       [[ 1.64466037e-05,  1.53698293e-05,  1.91599720e-05, ...,\n",
       "          1.76548729e-05,  1.70955263e-05,  1.41169767e-05],\n",
       "        [ 1.05826217e-07,  9.13145777e-06,  1.76140716e-07, ...,\n",
       "          1.21369967e-05,  1.55958387e-05,  1.23088676e-05],\n",
       "        [ 7.06836090e-06,  3.26818258e-06,  7.15508941e-06, ...,\n",
       "          4.64980394e-06,  8.96262554e-06,  6.55734584e-06],\n",
       "        [-3.18089551e-05, -8.93297238e-06, -2.51026504e-05, ...,\n",
       "         -7.02741545e-07,  1.34268867e-05,  3.24307348e-06]],\n",
       "\n",
       "       [[ 4.71092753e-05,  4.42863784e-05,  4.81883108e-05, ...,\n",
       "          5.11715079e-05,  4.54165129e-05,  4.68136932e-05],\n",
       "        [-2.25045878e-05, -1.77392903e-05, -2.29746092e-05, ...,\n",
       "         -1.16080564e-05, -1.33936874e-05, -1.49674725e-05],\n",
       "        [-7.06177207e-06, -3.24991493e-06, -3.82365158e-07, ...,\n",
       "         -2.51065717e-06, -8.64210142e-06, -1.25162501e-06],\n",
       "        [ 7.42783523e-05, -1.04295970e-04, -1.18935568e-04, ...,\n",
       "         -3.82503276e-05, -3.20346385e-06,  2.36954511e-05]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-9.19931699e-06, -1.12566586e-05, -6.25403072e-06, ...,\n",
       "         -6.18328158e-06, -7.30752890e-06, -5.60423996e-06],\n",
       "        [-8.76535523e-06, -4.28911699e-06, -5.81078848e-06, ...,\n",
       "         -3.45416925e-06, -7.85465454e-06, -3.31127535e-06],\n",
       "        [-3.47633295e-06,  1.70215810e-06, -5.54926203e-07, ...,\n",
       "          1.27369481e-06, -1.74354363e-06,  1.86796979e-06],\n",
       "        [ 2.13691851e-06,  1.01367950e-05,  1.14066588e-05, ...,\n",
       "         -1.72935925e-05, -1.24792969e-05, -1.97716799e-05]],\n",
       "\n",
       "       [[-3.31293450e-06,  8.98478265e-06,  3.39539952e-06, ...,\n",
       "         -6.63729168e-07, -1.82116059e-06, -4.22379713e-07],\n",
       "        [ 2.46191144e-06,  2.05640094e-06,  3.69124427e-06, ...,\n",
       "          3.03227239e-06,  5.80062397e-06,  2.60468755e-06],\n",
       "        [ 2.60216098e-06,  3.06390689e-06, -6.06109268e-07, ...,\n",
       "          8.91660693e-06, -2.67310270e-06,  7.77940932e-06],\n",
       "        [-1.12466259e-05, -7.33926136e-05, -1.92652078e-05, ...,\n",
       "          1.02715085e-05, -1.49204689e-05, -1.85605404e-05]],\n",
       "\n",
       "       [[-2.18067839e-06, -8.45159684e-06,  6.28021041e-07, ...,\n",
       "         -7.29226031e-06, -9.38796529e-06, -1.06312000e-05],\n",
       "        [ 5.12593260e-06,  6.90264642e-06,  6.00208568e-06, ...,\n",
       "          2.50897233e-06, -1.04963137e-05, -1.37404015e-06],\n",
       "        [ 7.75929493e-06,  1.52079519e-06,  7.64413744e-06, ...,\n",
       "          2.97681993e-07,  9.32359671e-06, -5.47995626e-07],\n",
       "        [-4.47002685e-06,  5.33304410e-06,  8.83690245e-06, ...,\n",
       "          1.00034840e-05,  4.90839875e-06,  1.29770433e-05]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv = np.load('data/LivingRoom_preprocessed_hack/Human1/deconvoled_trim.npy')\n",
    "human1_deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 4, 667200)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 3, 667200)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_deconv_3mic = human1_deconv[:, :3, :]\n",
    "human1_deconv_3mic.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3231.3293467 , -1127.87771457],\n",
       "       [-3198.54107875,  -744.5100656 ],\n",
       "       [-3192.9776274 ,  -248.26678827],\n",
       "       ...,\n",
       "       [-1717.89923578, -3166.59648491],\n",
       "       [-1808.60337549, -2779.13038427],\n",
       "       [   44.43741322,   106.48353609]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid = np.load('data/LivingRoom_preprocessed_hack/Human1/centroid.npy')\n",
    "human1_centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human1_centroid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(human1_deconv_3mic, human1_centroid, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.Tensor(X_train).cuda()\n",
    "X_test = torch.Tensor(X_test).cuda()\n",
    "\n",
    "y_train = torch.Tensor(y_train).cuda()\n",
    "y_test = torch.Tensor(y_test).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([800, 3, 667200])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "def audio_to_melspectrogram(audio, sample_rate=44100, n_mels=64, n_fft=2048, hop_length=512):\n",
    "    audio = torch.tensor(audio, dtype=torch.float32).cuda()\n",
    "    if audio.ndim == 1:\n",
    "        audio = audio.unsqueeze(0)\n",
    "    transformer = torchaudio.transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels).cuda()\n",
    "    melspectrogram = transformer(audio)\n",
    "    return melspectrogram.squeeze(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, audio_data, labels, transform=None):\n",
    "        self.audio_data = audio_data\n",
    "        _max = torch.Tensor([500, 2000]).cuda()\n",
    "        _min = torch.Tensor([-4000, -4000]).cuda()\n",
    "        self.labels = (labels - _min) / (_max - _min)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_signal = self.audio_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            audio_signal = self.transform(audio_signal)\n",
    "        return audio_signal, label\n",
    "\n",
    "def transform_to_melspectrogram(audio_signal):\n",
    "    # Convert the raw audio signal to a Mel spectrogram (or any other preprocessing)\n",
    "    melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32).cuda())\n",
    "    return melspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = AudioDataset(X_train, y_train, transform=transform_to_melspectrogram)\n",
    "test_dataset = AudioDataset(X_test, y_test, transform=transform_to_melspectrogram)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VGGishModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGGishModel, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        ).cuda()\n",
    "        # The output of features will depend on the input size, which should be calculated\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(165888, 4096),  # This dimension might need adjustment\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096, 2),  # Assume 1000 classes; adjust as needed\n",
    "            nn.Sigmoid()\n",
    "        ).cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1).cuda()\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24080/1795162868.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32).cuda())\n",
      "/tmp/ipykernel_24080/3240000509.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 0.055792086124420166, Validation Loss: 0.056696053880911604\n",
      "Epoch 2, Training Loss: 0.055349004864692686, Validation Loss: 0.05654160449138054\n",
      "Epoch 3, Training Loss: 0.0551870184391737, Validation Loss: 0.05634762490024933\n",
      "Epoch 4, Training Loss: 0.05523885443806648, Validation Loss: 0.05686298614511123\n",
      "Epoch 5, Training Loss: 0.055309867188334465, Validation Loss: 0.056307910153499015\n",
      "Epoch 6, Training Loss: 0.055185336247086526, Validation Loss: 0.05656950605603365\n",
      "Epoch 7, Training Loss: 0.055172324553132056, Validation Loss: 0.0564883050437157\n",
      "Epoch 8, Training Loss: 0.055227732583880425, Validation Loss: 0.05649129043404873\n",
      "Epoch 9, Training Loss: 0.05510018616914749, Validation Loss: 0.056427206557530626\n",
      "Epoch 10, Training Loss: 0.05379469357430935, Validation Loss: 0.04890080541372299\n",
      "Epoch 11, Training Loss: 0.052896439619362356, Validation Loss: 0.0569858316045541\n",
      "Epoch 12, Training Loss: 0.055067416727542874, Validation Loss: 0.05527057040196199\n",
      "Epoch 13, Training Loss: 0.051472459137439724, Validation Loss: 0.04371917075835741\n",
      "Epoch 14, Training Loss: 0.04398617561906576, Validation Loss: 0.0389249360618683\n",
      "Epoch 15, Training Loss: 0.04047258656471968, Validation Loss: 0.03581223787310032\n",
      "Epoch 16, Training Loss: 0.035501666367053986, Validation Loss: 0.03443451985143698\n",
      "Epoch 17, Training Loss: 0.033341938778758046, Validation Loss: 0.03359454383070652\n",
      "Epoch 18, Training Loss: 0.02520110543817282, Validation Loss: 0.019297663408976335\n",
      "Epoch 19, Training Loss: 0.018763536605983972, Validation Loss: 0.021209624237739123\n",
      "Epoch 20, Training Loss: 0.016045793946832417, Validation Loss: 0.015527470180621514\n",
      "Epoch 21, Training Loss: 0.014355644835159182, Validation Loss: 0.015158343916902175\n",
      "Epoch 22, Training Loss: 0.01233249381184578, Validation Loss: 0.012494925911036821\n",
      "Epoch 23, Training Loss: 0.012071634512394666, Validation Loss: 0.011947793026383106\n",
      "Epoch 24, Training Loss: 0.010108751663938164, Validation Loss: 0.01069875881792261\n",
      "Epoch 25, Training Loss: 0.008770962981507181, Validation Loss: 0.010684425512758585\n",
      "Epoch 26, Training Loss: 0.008676531482487917, Validation Loss: 0.00959808212848237\n",
      "Epoch 27, Training Loss: 0.007114812107756734, Validation Loss: 0.00825853760425861\n",
      "Epoch 28, Training Loss: 0.0069204348651692275, Validation Loss: 0.008180577182569183\n",
      "Epoch 29, Training Loss: 0.006215530335903168, Validation Loss: 0.007008595427928062\n",
      "Epoch 30, Training Loss: 0.0063422337174415585, Validation Loss: 0.010225460613862826\n",
      "Epoch 31, Training Loss: 0.005953802862204611, Validation Loss: 0.007574695037104762\n",
      "Epoch 32, Training Loss: 0.005925725973211229, Validation Loss: 0.007049673093626132\n",
      "Epoch 33, Training Loss: 0.004907462950795889, Validation Loss: 0.005797989057520261\n",
      "Epoch 34, Training Loss: 0.004924970041029156, Validation Loss: 0.005882350799555962\n",
      "Epoch 35, Training Loss: 0.0050275724241510035, Validation Loss: 0.005305317176792484\n",
      "Epoch 36, Training Loss: 0.004099627058021724, Validation Loss: 0.004706038269572533\n",
      "Epoch 37, Training Loss: 0.0034094167407602073, Validation Loss: 0.0041552777939404435\n",
      "Epoch 38, Training Loss: 0.0034470801800489427, Validation Loss: 0.005446550901979208\n",
      "Epoch 39, Training Loss: 0.0031274667219258843, Validation Loss: 0.0038838996910131895\n",
      "Epoch 40, Training Loss: 0.003140266411937773, Validation Loss: 0.0039146783081098245\n",
      "Epoch 41, Training Loss: 0.0029323182604275642, Validation Loss: 0.0038390552016118397\n",
      "Epoch 42, Training Loss: 0.002940134573727846, Validation Loss: 0.0035789585672318935\n",
      "Epoch 43, Training Loss: 0.00227210383862257, Validation Loss: 0.0030955415165338377\n",
      "Epoch 44, Training Loss: 0.0023706359206698835, Validation Loss: 0.0038677538440634427\n",
      "Epoch 45, Training Loss: 0.0023148006107658148, Validation Loss: 0.0027983166689339737\n",
      "Epoch 46, Training Loss: 0.0018683796899858862, Validation Loss: 0.003577750140371231\n",
      "Epoch 47, Training Loss: 0.00195611139992252, Validation Loss: 0.0030369728696174347\n",
      "Epoch 48, Training Loss: 0.001951839686371386, Validation Loss: 0.0029740879097237037\n",
      "Epoch 49, Training Loss: 0.0018242193188052625, Validation Loss: 0.0030549426939195166\n",
      "Epoch 50, Training Loss: 0.001534853063058108, Validation Loss: 0.002596237687752224\n",
      "Epoch 51, Training Loss: 0.0022140795877203345, Validation Loss: 0.003062749980017543\n",
      "Epoch 52, Training Loss: 0.0017298729333560913, Validation Loss: 0.0031088569577640067\n",
      "Epoch 53, Training Loss: 0.0015961798711214214, Validation Loss: 0.0027212624330646717\n",
      "Epoch 54, Training Loss: 0.0017783503839746118, Validation Loss: 0.002507539501843544\n",
      "Epoch 55, Training Loss: 0.0014719340903684497, Validation Loss: 0.0026557674729981674\n",
      "Epoch 56, Training Loss: 0.0015232993697281926, Validation Loss: 0.0026570576368472897\n",
      "Epoch 57, Training Loss: 0.0013813518651295453, Validation Loss: 0.0029030802946251174\n",
      "Epoch 58, Training Loss: 0.001430337050696835, Validation Loss: 0.0023066997886277162\n",
      "Epoch 59, Training Loss: 0.0013780097989365457, Validation Loss: 0.002291576806993152\n",
      "Epoch 60, Training Loss: 0.0014401578984688968, Validation Loss: 0.002246640004719106\n",
      "Epoch 61, Training Loss: 0.001193466063705273, Validation Loss: 0.0020664374236590588\n",
      "Epoch 62, Training Loss: 0.0012604432238731533, Validation Loss: 0.0022364718785796026\n",
      "Epoch 63, Training Loss: 0.0010901681275572628, Validation Loss: 0.002154775859358219\n",
      "Epoch 64, Training Loss: 0.0012887814897112547, Validation Loss: 0.003658965462818742\n",
      "Epoch 65, Training Loss: 0.0012205398490186781, Validation Loss: 0.00266983094983376\n",
      "Epoch 66, Training Loss: 0.0015930740698240698, Validation Loss: 0.0021455638970320043\n",
      "Epoch 67, Training Loss: 0.0009288958425167948, Validation Loss: 0.001954354471168839\n",
      "Epoch 68, Training Loss: 0.0010046996013261377, Validation Loss: 0.0021251665177540137\n",
      "Epoch 69, Training Loss: 0.0012374265096150338, Validation Loss: 0.0021369712015327355\n",
      "Epoch 70, Training Loss: 0.0010198953619692474, Validation Loss: 0.0022606149745675232\n",
      "Epoch 71, Training Loss: 0.0009758270846214146, Validation Loss: 0.0021988045818244037\n",
      "Epoch 72, Training Loss: 0.0010088211594847963, Validation Loss: 0.0018422686799357717\n",
      "Epoch 73, Training Loss: 0.0009629927843343467, Validation Loss: 0.001972615199450117\n",
      "Epoch 74, Training Loss: 0.0012506532727275044, Validation Loss: 0.002392260339827492\n",
      "Epoch 75, Training Loss: 0.0010978396772406995, Validation Loss: 0.0018317872353901083\n",
      "Epoch 76, Training Loss: 0.0007933868403779343, Validation Loss: 0.0018005758040369703\n",
      "Epoch 77, Training Loss: 0.0008613245951710268, Validation Loss: 0.001824445543416704\n",
      "Epoch 78, Training Loss: 0.0009112869563978165, Validation Loss: 0.0020449132246610066\n",
      "Epoch 79, Training Loss: 0.0009956142550799996, Validation Loss: 0.001909937637929733\n",
      "Epoch 80, Training Loss: 0.0010584175656549633, Validation Loss: 0.0022715125770236435\n",
      "Epoch 81, Training Loss: 0.0009247709607006982, Validation Loss: 0.001718182277937348\n",
      "Epoch 82, Training Loss: 0.0008902853255858645, Validation Loss: 0.0021621012890066663\n",
      "Epoch 83, Training Loss: 0.0007071922055911273, Validation Loss: 0.0020209769001947\n",
      "Epoch 84, Training Loss: 0.0006947122403653339, Validation Loss: 0.0017615460987704305\n",
      "Epoch 85, Training Loss: 0.0006788021590909921, Validation Loss: 0.0020539142692891452\n",
      "Epoch 86, Training Loss: 0.0006834164675092324, Validation Loss: 0.001832689830245307\n",
      "Epoch 87, Training Loss: 0.0006750311842188239, Validation Loss: 0.0017329128503871078\n",
      "Epoch 88, Training Loss: 0.0006508481974015013, Validation Loss: 0.0016776946632979582\n",
      "Epoch 89, Training Loss: 0.0008805831079371273, Validation Loss: 0.0018511969694652809\n",
      "Epoch 90, Training Loss: 0.0008119868842186406, Validation Loss: 0.0018081305226167808\n",
      "Epoch 91, Training Loss: 0.0007621226360788569, Validation Loss: 0.002202012164231676\n",
      "Epoch 92, Training Loss: 0.0006152020918671042, Validation Loss: 0.0015735796097522746\n",
      "Epoch 93, Training Loss: 0.0007978961674962192, Validation Loss: 0.001648136405632473\n",
      "Epoch 94, Training Loss: 0.0006132195735699497, Validation Loss: 0.0017935008195658715\n",
      "Epoch 95, Training Loss: 0.0006506819772766903, Validation Loss: 0.0015551651102634003\n",
      "Epoch 96, Training Loss: 0.0006071031652390957, Validation Loss: 0.0017348021412125002\n",
      "Epoch 97, Training Loss: 0.0006759347859770059, Validation Loss: 0.0016586358318678462\n",
      "Epoch 98, Training Loss: 0.0005838091718032956, Validation Loss: 0.0016566734904280077\n",
      "Epoch 99, Training Loss: 0.0007053861347958445, Validation Loss: 0.0020008706129514254\n",
      "Epoch 100, Training Loss: 0.0007357164210407063, Validation Loss: 0.0016035126531138443\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VGGishModel().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training and validation loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for spectrograms, coordinates in train_loader:\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        coordinates = coordinates.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(spectrograms)\n",
    "        loss = criterion(outputs, coordinates)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()\n",
    "    validation_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for spectrograms, coordinates in test_loader:\n",
    "            spectrograms = spectrograms.to(device)\n",
    "            coordinates = coordinates.to(device)\n",
    "\n",
    "            outputs = model(spectrograms)\n",
    "            loss = criterion(outputs, coordinates)\n",
    "            validation_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}, Validation Loss: {validation_loss / len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24080/1795162868.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  melspec = audio_to_melspectrogram(torch.tensor(audio_signal, dtype=torch.float32).cuda())\n",
      "/tmp/ipykernel_24080/3240000509.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(audio, dtype=torch.float32).cuda()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Distance Error: 123.38296189385692\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "_max = np.array([500, 2000])\n",
    "_min = np.array([-4000, -4000])\n",
    "\n",
    "def unnormalize(x):\n",
    "    return (x + 1) / 2 * (_max - _min) + _min\n",
    "\n",
    "distance_errors = []\n",
    "\n",
    "for spectrograms, actual_coordinates in test_loader:\n",
    "    spectrograms = spectrograms.to(device)\n",
    "    actual_coordinates = actual_coordinates.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predicted_coordinates = model(spectrograms)\n",
    "        predicted_coordinates = predicted_coordinates.cpu().numpy()\n",
    "        actual_coordinates = actual_coordinates.cpu().numpy()\n",
    "    \n",
    "    for pred, actual in zip(predicted_coordinates, actual_coordinates):\n",
    "        pred_unnorm = unnormalize(pred)\n",
    "        actual_unnorm = unnormalize(actual)\n",
    "        \n",
    "        distance = np.linalg.norm(pred_unnorm - actual_unnorm)\n",
    "        distance_errors.append(distance)\n",
    "\n",
    "average_distance_error = np.mean(distance_errors)\n",
    "\n",
    "print(\"Average Distance Error:\", average_distance_error)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
